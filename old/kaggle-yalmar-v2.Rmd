---
title: "R Notebook"
output: html_notebook
---
```{r}
rm(list = ls())
```


```{r}
# Cargando librerias
# ==============================================================================
library(skimr)
library(caret)
library(lattice)
library(ggplot2)
library(VIM)
library(reshape2)
library(randomForest)
library(caret)
library(doSNOW)
library(doParallel)
```


```{r}
# Cargando la data
# ==============================================================================
train<- read.csv("train.csv", sep=",")
test<- read.csv("test.csv", sep = ",")
```

```{r}
# visualizando la data
# ==============================================================================
skim(train)
#skim(test)

```
```{r}
# Imputacion de datos faltantes
# ==============================================================================
kn=5

train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset( train, select = -c(total_bedrooms_imp) )
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )
```

```{r}
# visualizando la data despues de realizar la imputacion
# ==============================================================================
skim(train)
skim(test)
```
# grafica boxpplot para visualizar los outliers
```{r}
# visualizando de los outliers
# ==============================================================================
subData<-train %>%
  dplyr::select(colnames(train[,-10]))
dataX<-melt(subData)

dataX %>%
  ggplot(aes(x=variable, y=value)) +
  geom_boxplot()

subData<-test %>%
  dplyr::select(colnames(test[,-10]))
dataX2<-melt(subData)

dataX2 %>%
  ggplot(aes(x=variable, y=value)) +
  geom_boxplot()
```
```{r}
# Reemplazo de outlier por el metodo knn de los datos de entrenamiento
# ==============================================================================
outliers <- boxplot(train$total_rooms, plot = FALSE)$out
train[train$total_rooms %in% outliers, "total_rooms"] = NA
train<- kNN(train,variable=c("total_rooms"),k=kn)
train<- subset( train, select = -c(total_rooms_imp) )
 
outliers <- boxplot(train$total_bedrooms, plot = FALSE)$out
train[train$total_bedrooms %in% outliers, "total_bedrooms"] = NA
train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset(train, select = -c(total_bedrooms_imp) )

outliers <- boxplot(train$population, plot = FALSE)$out
train[train$population %in% outliers, "population"] = NA
train<- kNN(train,variable=c("population"),k=kn)
train<- subset(train, select = -c(population_imp) )

outliers <- boxplot(train$households, plot = FALSE)$out
train[train$households %in% outliers, "households"] = NA
train<- kNN(train,variable=c("households"),k=kn)
train<- subset(train, select = -c(households_imp) )

outliers <- boxplot(train$median_house_value, plot = FALSE)$out
train[train$median_house_value %in% outliers, "median_house_value"] = NA
train<- kNN(train,variable=c("median_house_value"),k=kn)
train<- subset(train, select = -c(median_house_value_imp) )


# Reemplazo de outlier por el metodo knn de los datos de prueba
# ==============================================================================

outliers <- boxplot(test$total_rooms, plot = FALSE)$out
test[test$total_rooms %in% outliers, "total_rooms"] = NA
test<- kNN(test,variable=c("total_rooms"),k=kn)
test<- subset(test, select = -c(total_rooms_imp) )

outliers <- boxplot(test$total_bedrooms, plot = FALSE)$out
test[test$total_bedrooms %in% outliers, "total_bedrooms"] = NA
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )

outliers <- boxplot(test$population, plot = FALSE)$out
test[test$population %in% outliers, "population"] = NA
test<- kNN(test,variable=c("population"),k=kn)
test<- subset(test, select = -c(population_imp) )

outliers <- boxplot(test$households, plot = FALSE)$out
test[test$households %in% outliers, "households"] = NA
test<- kNN(test,variable=c("households"),k=kn)
test<- subset( test, select = -c(households_imp) )
```



```{r}
# visualizando de los outliers
# ==============================================================================
subData<-train %>%
  dplyr::select(colnames(train[,-10]))
dataX<-melt(subData)
dataX %>%
  ggplot(aes(x=variable, y=value)) +
  geom_boxplot()

subData<-test %>%
  dplyr::select(colnames(test[,-10]))
dataX2<-melt(subData)
dataX2 %>%
  ggplot(aes(x=variable, y=value)) +
  geom_boxplot()
```

```{r}
# Tranformaciones de variables categoricas
# ==============================================================================
subData<- train %>%
  dplyr::select(ocean_proximity)

OHE<-dummyVars("~.", data=train)
train<-data.frame(predict(OHE, newdata=train))
train

subData2<- test %>%
  dplyr::select(ocean_proximity)

OHE<-dummyVars("~.", data=test)
test<-data.frame(predict(OHE, newdata=test))
test

# Tratamiendo de variables
#train$total_bedrooms>-log(train$total_bedrooms)
#test$total_bedrooms>-log(test$total_bedrooms)

```

```{r}
skim(train)
```



```{r}
# Models
# ==============================================================================
## Random Forest Regression
# ==============================================================================

rf.label <- as.factor(train$median_house_value)

set.seed(1234)
rf.1 <- randomForest(x = train[-c(10)], y = train$median_house_value, importance = TRUE, ntree = 1000)
rf.1
varImpPlot(rf.1)

```
```{r}
sqrt(rf.1$mse[length(rf.1$mse)])
train
rf.1
```
```{r}
# Prediccion usando Random Forest y guardado de la data en CSV.
# ==============================================================================



predictions <- rf.1 %>% predict(test)

predictions<-data.frame(predictions)
predictions$id <-test$id
colnames(predictions)[colnames(predictions) == 'predictions'] <- 'median_house_value'

predictions<-predictions %>%
  dplyr::select(id,median_house_value)
predictions

write.csv(x = predictions, file = "test_predict.csv", row.names = FALSE) 
```



```{r}
# Random Forest Model using Ranger
# ==============================================================================

# librerias
# ==============================================================================
library(randomForest)
library(caret)
library(e1071)
library(ranger)

# Corriendo el Modelo
# ==============================================================================
set.seed(1234)
train_2<-train[-c(13)]
rf_classifier <- ranger(median_house_value ~ .,
                        data=train_2,
                        num.trees=1000,
                        write.forest=TRUE,
                        min.node.size=2,
                        seed=123,
                        mtry=5,importance= 'impurity')

sqrt(rf_classifier$prediction.error)
rf_classifier
train_2
#rf_classifier$variable.importance


list_features=data.frame("feature"= names(rf_classifier$variable.importance),"value"=unname(rf_classifier$variable.importance))

list_features %>%
dplyr::arrange(desc(value)) %>%
dplyr::top_n(25) %>%
ggplot(aes(reorder(feature, value), value)) +
geom_col() +
coord_flip() +
ggtitle("Top important variables")
```
```{r}
pp <- predict(rf_classifier,test)
out <- data.frame(
    id=test$id,
    median_house_value=pp$predictions,row.names=NULL)

write.csv(x = out, file = "test_predict.csv", row.names = FALSE)

```


```{r}
rm(list = ls())
# LIBRERIAS Y DATOS
# -----------------------------------------------------
library(MASS); library(neuralnet); library(ggplot2)
train<- read.csv("train.csv", sep=",")
test<- read.csv("test.csv", sep = ",")

kn=5

train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset( train, select = -c(total_bedrooms_imp) )
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )


# Reemplazo de outlier por el metodo knn de los datos de entrenamiento
# ==============================================================================
outliers <- boxplot(train$total_rooms, plot = FALSE)$out
train[train$total_rooms %in% outliers, "total_rooms"] = NA
train<- kNN(train,variable=c("total_rooms"),k=kn)
train<- subset( train, select = -c(total_rooms_imp) )
 
outliers <- boxplot(train$total_bedrooms, plot = FALSE)$out
train[train$total_bedrooms %in% outliers, "total_bedrooms"] = NA
train<- kNN(train,variable=c("total_bedrooms"),k=kn)
train<- subset(train, select = -c(total_bedrooms_imp) )

outliers <- boxplot(train$population, plot = FALSE)$out
train[train$population %in% outliers, "population"] = NA
train<- kNN(train,variable=c("population"),k=kn)
train<- subset(train, select = -c(population_imp) )

outliers <- boxplot(train$households, plot = FALSE)$out
train[train$households %in% outliers, "households"] = NA
train<- kNN(train,variable=c("households"),k=kn)
train<- subset(train, select = -c(households_imp) )

outliers <- boxplot(train$median_house_value, plot = FALSE)$out
train[train$median_house_value %in% outliers, "median_house_value"] = NA
train<- kNN(train,variable=c("median_house_value"),k=kn)
train<- subset(train, select = -c(median_house_value_imp) )


# Reemplazo de outlier por el metodo knn de los datos de prueba
# ==============================================================================

outliers <- boxplot(test$total_rooms, plot = FALSE)$out
test[test$total_rooms %in% outliers, "total_rooms"] = NA
test<- kNN(test,variable=c("total_rooms"),k=kn)
test<- subset(test, select = -c(total_rooms_imp) )

outliers <- boxplot(test$total_bedrooms, plot = FALSE)$out
test[test$total_bedrooms %in% outliers, "total_bedrooms"] = NA
test<- kNN(test,variable=c("total_bedrooms"),k=kn)
test<- subset(test, select = -c(total_bedrooms_imp) )

outliers <- boxplot(test$population, plot = FALSE)$out
test[test$population %in% outliers, "population"] = NA
test<- kNN(test,variable=c("population"),k=kn)
test<- subset(test, select = -c(population_imp) )

outliers <- boxplot(test$households, plot = FALSE)$out
test[test$households %in% outliers, "households"] = NA
test<- kNN(test,variable=c("households"),k=kn)
test<- subset( test, select = -c(households_imp) )



subData<- train %>%
  dplyr::select(ocean_proximity)

OHE<-dummyVars("~.", data=train)
train<-data.frame(predict(OHE, newdata=train))


subData2<- test %>%
  dplyr::select(ocean_proximity)

OHE<-dummyVars("~.", data=test)
test<-data.frame(predict(OHE, newdata=test))


```



```{r}

set.seed(65)
datos    <- train
n        <- nrow(datos)
muestra  <- sample(n, n * .70)
train    <- datos[muestra, ]
test     <- datos[-muestra, ]
 
 
# NORMALIZACION DE VARIABLES
# -----------------------------------------------------
maxs      <- apply(train, 2, max)
mins      <- apply(train, 2, min)
datos_nrm <- as.data.frame(scale(datos, center = mins, scale = maxs - mins))
train_nrm <- datos_nrm[muestra, ]
test_nrm  <- datos_nrm[-muestra, ]
 
 
# FORMULA
# -----------------------------------------------------
nms  <- names(train_nrm)
frml <- as.formula(paste("median_house_value ~", paste(nms[!nms %in% "median_house_value"], collapse = " + ")))
 
 
# MODELO
# -----------------------------------------------------
modelo.nn <- neuralnet(frml,
                       data          = train_nrm,
                       hidden        = c(7,5), # ver Notas para detalle 
                       threshold     = 0.05,   # ver Notas para detalle
                       algorithm     = "rprop+" 
                       )
 
 

```





```{r}
# PREDICCION
# -----------------------------------------------------
pr.nn   <- compute(modelo.nn,within(test_nrm,rm(median_house_value)))
 
# se transoforma el valor escalar al valor nominal original
medv.predict <- pr.nn$net.result*(max(datos$median_house_value)-min(datos$median_house_value))+min(datos$median_house_value)
medv.real    <- (test_nrm$median_house_value)*(max(datos$median_house_value)-min(datos$median_house_value))+min(datos$median_house_value)
 
 
 
# SUMA DE ERROR CUADRATICO
# -----------------------------------------------------
(se.nn <- sum((medv.real - medv.predict)^2)/nrow(test_nrm))
 
 
#GRAFICOS
# -----------------------------------------------------
# Errores
qplot(x=medv.real, y=medv.predict, geom=c("point","smooth"), method="lm", 
      main=paste("Real Vs Prediccion. Summa de Error Cuadratico=", round(se.nn,2)))
# Red
plot(modelo.nn)
sqrt(se.nn)
```






